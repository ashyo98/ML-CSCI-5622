{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import data\n",
    "import tests\n",
    "%matplotlib inline\n",
    "\n",
    "house_prices = data.HousePrices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMaxScaler:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.min = None\n",
    "        self.max = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Compute and save the features min and max of shape (num_features,)\n",
    "        (you can also save them with shape (1, num_features))\n",
    "        :param X: array of shape (num_samples, num_features)\n",
    "        :return: fitted scaler\n",
    "        \"\"\"\n",
    "        # Workspace 1.1.a\n",
    "        #BEGIN \n",
    "        # code here\n",
    "        self.min = np.min(X, axis=0)\n",
    "        self.max = np.max(X, axis=0)\n",
    "        #END\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform the given samples using the precomputed min and max\n",
    "        :param X: np.array of shape (num_samples, num_features)\n",
    "        :return: MinMax scaled X, of shape (num_samples, num_features)\n",
    "        \"\"\"\n",
    "        # Workspace 1.1.b\n",
    "        #BEGIN \n",
    "        # code here\n",
    "        for feature in range(X.shape[1]):\n",
    "            X[:, feature] = (X[:, feature] - self.min[feature]) / (self.max[feature] - self.min[feature])\n",
    "        return X\n",
    "        #END\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"\n",
    "        Fit using X and then transform it. Useful when we need to scale jsut once.\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_prices = data.HousePrices()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "# MinMax works on 2-d arrays, so we just need to parse the prices as a single column/feature\n",
    "# and then squeeze it back to 1-d array\n",
    "house_prices.y_train = minmax_scaler.fit_transform(house_prices.y_train[:, None])[:, 0]\n",
    "house_prices.y_test = minmax_scaler.transform(house_prices.y_test[:, None])[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.40232227  0.34984652  0.29730165]\n",
      " [ 0.07629464  1.78215722 -0.25926978]\n",
      " [-0.02383004 -1.35726955  0.89944945]\n",
      " [-1.45478685 -0.7747342  -0.93748131]]\n",
      "[[ 1.40232227  0.07629464 -0.02383004 -1.45478685]\n",
      " [ 0.34984652  1.78215722 -1.35726955 -0.7747342 ]\n",
      " [ 0.29730165 -0.25926978  0.89944945 -0.93748131]]\n",
      "[[ 0.67220542 -0.24255507 -0.58161048]\n",
      " [-0.24255507  0.2226884   0.16951212]\n",
      " [-0.58161048  0.16951212  0.77545272]]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features = np.array([[1.55143777, 0.2644804, 0.0995576],\n",
    "                         [0.22541014, 1.6967911, -0.45701382],\n",
    "                         [0.12528546, -1.44263567, 0.7017054],\n",
    "                         [-1.30567135, -0.86010032, -1.13522536]])\n",
    "labels = np.array([136.70039877, 10.1003086, 44.67363091, -221.48398972])\n",
    "\n",
    "X_hat = np.empty((features.shape[0], features.shape[1]))\n",
    "X_hat = features - np.mean(features, axis=0)\n",
    "\n",
    "# print(x_mean)\n",
    "print(X_hat)\n",
    "print(X_hat.T)\n",
    "# print(alpha_matrix)\n",
    "\n",
    "print(np.linalg.inv(np.transpose(X_hat).dot(X_hat) + 2))\n",
    "print(\"\\n\\n\")\n",
    "# print(np.linalg.inv(np.transpose(X_hat).dot(X_hat) + alpha_matrix))\n",
    "# print(np.transpose(X_hat).dot(labels))\n",
    "\n",
    "# print(features[:, 0])\n",
    "# print(features[:, 0] - 1)\n",
    "# print(np.subtract(features[:, 0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "[[ 3.15251974e+15 -6.30503948e+15  3.15251974e+15]\n",
      " [-6.30503948e+15  1.26100790e+16 -6.30503948e+15]\n",
      " [ 3.15251974e+15 -6.30503948e+15  3.15251974e+15]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1,2,3], [4,5,6], [7,8,9]])\n",
    "print(a)\n",
    "print(np.linalg.inv(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 0. 0.]\n",
      " [0. 2. 0.]\n",
      " [0. 0. 2.]]\n"
     ]
    }
   ],
   "source": [
    "I = np.identity(features.shape[1])\n",
    "print( 2 * I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1)\n",
      "[[0.        ]\n",
      " [0.54231535]\n",
      " [0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import lasso_path\n",
    "X = np.array([[1, 2, 3.1, 4.5], [2.3, 5.4, 4.3, 5.7], [1,2,3,4]]).T\n",
    "y = np.array([1, 2, 3.1, 7])\n",
    "_, coef_path, _ = lasso_path(X, y, alphas=[5.])\n",
    "print(coef_path.shape)\n",
    "print(coef_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16.1549399]]\n",
      "[[0.9999999]]\n"
     ]
    }
   ],
   "source": [
    "a=np.array([[16.154939900325]])\n",
    "a[a > 25] = 25\n",
    "a[a < -25] = -25\n",
    "print(a)\n",
    "ans=1 / (1 + np.exp(-a))\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "[[1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "a=np.array([1,2])\n",
    "a=a.reshape(2,1)\n",
    "print(a.shape)\n",
    "b=np.array([[0],[1]])\n",
    "print(b.shape)\n",
    "\n",
    "c=a-b\n",
    "print(c.shape)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [1 0 0 0]\n",
      " [0 0 1 0]\n",
      " [0 0 0 1]\n",
      " [0 0 1 0]\n",
      " [0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "y=np.array([0,1,1,1,0,2,3,2,3])\n",
    "label_to_index=dict(zip(np.unique(y), range(len(np.unique(y)))))\n",
    "index_to_label = {v: k for k, v in label_to_index.items()}\n",
    "\n",
    "one_hot_encode = np.zeros((y.shape[0], len(label_to_index)), dtype=int)\n",
    "for i in range(len(y)): # iterate over each sample\n",
    "    for j in label_to_index.keys(): # iterate over each class_label\n",
    "        if y[i] == j:\n",
    "            one_hot_encode[i][label_to_index[j]] = 1\n",
    "\n",
    "print(one_hot_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "[1. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "Y_onehot = onehot_encoder.fit_transform(y.reshape(-1,1))\n",
    "print(Y_onehot)\n",
    "print(Y_onehot[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [1 0 0 0]\n",
      " [0 0 1 0]\n",
      " [0 0 0 1]\n",
      " [0 0 1 0]\n",
      " [0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "one_hot_encode_2 = np.zeros((y.shape[0], len(label_to_index)), dtype=int)\n",
    "for ind, val in enumerate(y):\n",
    "    one_hot_encode_2[ind][val] = 1\n",
    "\n",
    "print(one_hot_encode_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeSelf(object):\n",
    "\n",
    "    def __init__(self, alpha, normalize=False):\n",
    "        \"\"\"\n",
    "        :param alpha: regularization parameter\n",
    "        :param normalize: boolean whether to normalize the features or not\n",
    "        \"\"\"\n",
    "\n",
    "        self.alpha = alpha  # our tuning / regularization parameter\n",
    "        self.coefficients = None  # our weights vector, w (in formulae above)\n",
    "        self.intercept = None  # our intercept parameter, b (in formulae above)\n",
    "        self.normalize = normalize  # boolean whether to normalize the features or not\n",
    "        self.scaler = StandardScaler()  # method by which to normalize the features (depends on self.normalize)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the ridge model, train it using the provided data\n",
    "        Calculate the number of non-zero coefficients in the model weights and the norm using np.linalg.norm\n",
    "        :param X: training features (num_samples, num_features)\n",
    "        :param y: target values (num_samples)\n",
    "        :return: tuple (number of non-zeros coefficients of w, norm of w)\n",
    "        \"\"\"\n",
    "        num_nonzero_coefs, coef_norm = 0, 0\n",
    "        # Workspace 1.2.a\n",
    "        # TO DO: compute w and b and store them in self.coef_ and self.intercept\n",
    "        # HINT: use self.scaler first, if and only if self.normalize is True\n",
    "        #BEGIN \n",
    "        # code here\n",
    "        \n",
    "        # normalize is required\n",
    "        if self.normalize == True:\n",
    "            self.scaler.fit(X)\n",
    "            X = self.scaler.transform(X)\n",
    "            # X = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # compute X_hat\n",
    "        X_hat = np.empty((X.shape[0], X.shape[1]))\n",
    "        X_hat = X - np.mean(X, axis=0)\n",
    "        I = np.identity(X.shape[1])\n",
    "        \n",
    "        # compute coef vector\n",
    "        w_star = np.empty(X.shape[0])\n",
    "        # print(\"x * x^T\", (X_hat.dot(np.transpose(X_hat))).shape)\n",
    "        # print(\"deno shape\", (X_hat.dot(np.transpose(X_hat)) + alpha_matrix).shape)\n",
    "        # print(\"1/deno shape\", (1 / (X_hat.dot(np.transpose(X_hat)) + alpha_matrix) ).shape)\n",
    "        # print(\"x^T * y\", (np.transpose(X_hat).dot(y)).shape)\n",
    "        w_star = (np.linalg.inv(np.transpose(X_hat).dot(X_hat) + self.alpha * I)).dot(np.transpose(X_hat)).dot(y)\n",
    "        # w_star = np.dot(np.dot(np.linalg.inv(np.dot(X_hat.T, X_hat) + self.alpha * I), X_hat.T), y)\n",
    "        self.coefficients = w_star\n",
    "\n",
    "        # computer intercept\n",
    "        sum = 0\n",
    "        for sample in range(len(X)):\n",
    "            sum += (y[sample] - (np.transpose(w_star).dot(X[sample])))\n",
    "        self.intercept = sum/len(X)\n",
    "\n",
    "        num_nonzero_coefs = np.count_nonzero(self.coefficients)\n",
    "        coef_norm = np.linalg.norm(self.coefficients)\n",
    "        #END\n",
    "        return num_nonzero_coefs, coef_norm\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute Root mean square error (RMSE) between the predicted values and the actual values of the test data\n",
    "        :param X: instances array of shape (num_samples, num_features)\n",
    "        :param y: the true targets, of shape (num_samples)\n",
    "        :return: RMSE\n",
    "        \"\"\"\n",
    "\n",
    "        # Workspace 1.2.b\n",
    "        # TO DO: predict based on the test features and return the root mean squared error\n",
    "        #BEGIN \n",
    "        # code here\n",
    "        if self.normalize == True: # normalize the test data\n",
    "            X = self.scaler.transform(X)\n",
    "        y_hat = np.empty(y.shape)\n",
    "        for sample in range(len(X)):\n",
    "            y_hat[sample] = np.transpose(self.coefficients).dot(X[sample]) + self.intercept\n",
    "        \n",
    "        # y_hat = np.transpose(self.coefficients).dot(X) + self.intercept\n",
    "        \n",
    "        root_mean_squared_error = np.sqrt(np.mean((y_hat - y) ** 2))\n",
    "        \n",
    "        #END\n",
    "        return root_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00065838  0.00442683  0.00892526 -0.00020149  0.00158773  0.00465773\n",
      "  0.00528186  0.00176966  0.01096399  0.00757301  0.00443479 -0.00594338]\n",
      "0.061247813321450045\n",
      "0.028003667392365386\n"
     ]
    }
   ],
   "source": [
    "ri = RidgeSelf(1e4, normalize=True)\n",
    "ri.fit(house_prices.X_train, house_prices.y_train)\n",
    "\n",
    "print(ri.coefficients)\n",
    "print(ri.intercept)\n",
    "\n",
    "print(ri.evaluate(house_prices.X_test, house_prices.y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.026894935550812665, 0.026895024833162473, 0.026895136532495877, 0.026895360250842715, 0.026895584392564027, 0.026897164954318042, 0.026918870561759442, 0.027124816295421943, 0.027333759286632564, 0.027414780350526323, 0.02749422357803399, 0.027536368618381258, 0.027575056257770066, 0.02786459099894152, 0.028388629911411965]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "scaler = StandardScaler()\n",
    "alphas = [0.01, 0.05, 0.1, 0.2, 0.3, 1.0, 10.0, 100.0, 300.0, 500.0, 1e3, 1.5e3, 2e3, 5e3, 1e4]\n",
    "rmses = []\n",
    "\n",
    "for alpha in alphas:\n",
    "\n",
    "    X = scaler.fit_transform(house_prices.X_train)\n",
    "    X = house_prices.X_train\n",
    "    y = house_prices.y_train\n",
    "\n",
    "    r = Ridge(alpha=alpha)\n",
    "    r.fit(X, y)\n",
    "\n",
    "    X_test = scaler.transform(house_prices.X_test)\n",
    "    X_test = house_prices.X_test\n",
    "\n",
    "    # print(r.coef_)\n",
    "    # print(r.intercept_)\n",
    "\n",
    "    y_hat = r.predict(X_test)\n",
    "    rmse = np.sqrt(np.mean((y_hat - house_prices.y_test) ** 2))\n",
    "    rmses.append(rmse)\n",
    "\n",
    "print(rmses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02689491 0.02689491 0.0268949  0.02689488 0.02689486 0.02689475\n",
      " 0.02689329 0.02687956 0.0268543  0.02683553 0.02681214 0.02681517\n",
      " 0.02683797 0.02719086 0.02800367]\n"
     ]
    }
   ],
   "source": [
    "a=np.array([0.026894911608571895, 0.026894905078820863, 0.02689489691715849, 0.026894880595324112, 0.026894864275439887, 0.026894750092151743, 0.026893290679514736, 0.026879560001868665, 0.026854299069622667, 0.02683553442791724, 0.026812139357760303, 0.02681517129952617, 0.026837971053934276, 0.027190858357845734, 0.028003667392365386])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import lasso_path\n",
    "\n",
    "\n",
    "class LassoSelf(object):\n",
    "    def __init__(self, alpha, normalize=False):\n",
    "        \"\"\"\n",
    "        :param alpha: regularization parameter\n",
    "        :param normalize: boolean whether to normalize the features or not\n",
    "        \"\"\"\n",
    "        self.alpha = alpha  # our tuning / regularization parameter\n",
    "        self.coefficients = None  # our weights vector, w (in formulae above)\n",
    "        self.intercept = None  # our intercept parameter, b (in formulae above)\n",
    "        self.normalize = normalize  # boolean whether to normalize the features or not\n",
    "        self.scaler = StandardScaler()  # method by which to normalize the features (depends on self.normalize)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the lasso model, train it using the provided data\n",
    "        Calculate the number of non-zero coefficients in the model weights and the norm using np.linalg.norm\n",
    "        :param X: training features (num_samples, num_features)\n",
    "        :param y: target values (num_samples)\n",
    "        :return: tuple (number of non-zeros coefficients of w: scalar, norm of w: scalar)\n",
    "        \"\"\"\n",
    "\n",
    "        num_nonzero_coefs, coef_norm = 0, 0\n",
    "        # Workspace 1.4.a\n",
    "        # TO DO: compute w and b and store then in self.coef_ and self.intercept\n",
    "        # TO DO: call lasso_path on the centered features to compute self.coef_\n",
    "        # HINT: use self.scaler first, if and only if self.normalize is True\n",
    "        #BEGIN \n",
    "        # code here\n",
    "        # normalize is required\n",
    "        if self.normalize == True:\n",
    "            self.scaler.fit(X)\n",
    "            X = self.scaler.transform(X)\n",
    "                \n",
    "        # compute X_hat\n",
    "        X_hat = np.empty((X.shape[0], X.shape[1]))\n",
    "        X_hat = X - np.mean(X, axis=0)\n",
    "\n",
    "        _, w_star, _ = lasso_path(X_hat, y, alphas=[self.alpha])\n",
    "\n",
    "        self.coefficients = w_star.T\n",
    "\n",
    "        # computer intercept\n",
    "        sum = 0\n",
    "        for sample in range(len(X)):\n",
    "            sum += (y[sample] - (np.transpose(w_star).dot(X[sample])))\n",
    "        self.intercept = sum/len(X)\n",
    "\n",
    "        num_nonzero_coefs = np.count_nonzero(self.coefficients)\n",
    "        coef_norm = np.linalg.norm(self.coefficients)\n",
    "        #END\n",
    "        return num_nonzero_coefs, coef_norm\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute Root mean square error (RMSE) between the predicted values and the actual values  of the test data\n",
    "        :param X: features array, shape (num_samples, num_features)\n",
    "        :param y: true targets, shape (num_samples)\n",
    "        :return: RMSE\n",
    "        \"\"\"\n",
    "        root_mean_squared_error = 0\n",
    "        # Workspace 1.4.b\n",
    "        # TO DO: predict based on the test features and return the mean_squared_error\n",
    "        #BEGIN \n",
    "        # code here\n",
    "        if self.normalize == True: # normalize the test data\n",
    "            X = self.scaler.transform(X)\n",
    "        y_hat = np.empty(y.shape)\n",
    "        for sample in range(len(X)):\n",
    "            # y_hat[sample] = np.transpose(self.coefficients).dot(X[sample]) + self.intercept\n",
    "            y_hat[sample] = self.coefficients.dot(X[sample]) + self.intercept\n",
    "                \n",
    "        root_mean_squared_error = np.sqrt(np.mean((y_hat - y) ** 2))\n",
    "        #END\n",
    "        return root_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00000000e+00  0.00000000e+00  3.77327633e-05 -3.55267717e-08\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  4.49571972e-07 -4.43693966e-05]]\n",
      "[0.07052146]\n"
     ]
    }
   ],
   "source": [
    "ri = LassoSelf(0.2, normalize=False)\n",
    "ri.fit(house_prices.X_train, house_prices.y_train)\n",
    "\n",
    "print(ri.coefficients)\n",
    "print(ri.intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00000000e+00  0.00000000e+00  3.77327633e-05 -3.55267717e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  4.49571972e-07 -4.43693966e-05]\n",
      "0.07052146115486504\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# X = scaler.fit_transform(house_prices.X_train)\n",
    "X = house_prices.X_train\n",
    "y = house_prices.y_train\n",
    "\n",
    "l = Lasso(alpha=0.2)\n",
    "l.fit(X, y)\n",
    "\n",
    "# X_test = scaler.transform(house_prices.X_test)\n",
    "\n",
    "print(l.coef_)\n",
    "print(l.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "class LogisticRegressionSelf:\n",
    "\n",
    "    def __init__(self, eta=0.1, alpha=0):\n",
    "        \"\"\"\n",
    "        Create a logistic regression classifier\n",
    "        :param eta: Learning rate\n",
    "        :param alpha: We will use this parameter later (IN BONUS)\n",
    "        \"\"\"\n",
    "\n",
    "        self.w = None  # uninitialized w\n",
    "        self.eta = eta  # learning rate\n",
    "        self.initialized = False # flag used to initialize w only once, it allows calling fit multiple times\n",
    "        self.alpha = alpha  # regularization / penalty term (USED IN BONUS)\n",
    "\n",
    "    def sigmoid(self, x, threshold=25.0):\n",
    "        \"\"\"\n",
    "\n",
    "        :param x: features array of shape (num_samples, num_features + 1) (zero-th column appended)\n",
    "        :param threshold: the truncating threshold for np.exp, default to 25.0\n",
    "        :return: sigmoid values , of shape (num_samples,)\n",
    "        \"\"\"\n",
    "        # Workspace 2.1\n",
    "        # TO DO: Complete this function to return the output of applying the sigmoid function to the score\n",
    "        #BEGIN \n",
    "        # code here\n",
    "        # print(\"x shape is: \", x.shape)\n",
    "        # print(\"w shape is: \", self.w.shape)\n",
    "        sigmoid_val = np.empty((len(x),))\n",
    "        # for i in range(len(x)):\n",
    "        #     sigmoid_val[i] = np.dot(np.transpose(self.w), x[i])\n",
    "        sigmoid_val = np.dot(x, self.w)\n",
    "        # print(\"sigmoid_val is: \", sigmoid_val)\n",
    "        sigmoid_val[sigmoid_val > threshold] = threshold\n",
    "        sigmoid_val[sigmoid_val < -threshold] = -threshold\n",
    "\n",
    "        # for i in range(len(sigmoid_val)):\n",
    "        #     sigmoid_val[i] = 1 / (1 + np.exp(-sigmoid_val[i]))\n",
    "        \n",
    "        return 1 / (1 + np.exp(-sigmoid_val))\n",
    "        #END\n",
    "\n",
    "    def compute_gradient(self, x, y):\n",
    "        \"\"\"\n",
    "        Return the derivative of the cost w.r.t to the weights. Don't forget to average by batch_size\n",
    "        :param x:  Feature vector, shape (batch_size, num_features +1), with zero-th column appended\n",
    "        :param y: real binary class label, shape (batch_size)\n",
    "        :return: gradient of shape (num_features + 1,)\n",
    "        \"\"\"\n",
    "\n",
    "        # Workspace 2.2\n",
    "        # TO DO: Finish this function to compute the gradient\n",
    "        gradient = np.zeros((x.shape[1], ))\n",
    "        #BEGIN \n",
    "        # code here\n",
    "        sigmoid_val = self.sigmoid(x)\n",
    "        # print(\"sigmoid_val-y shape is: \", (sigmoid_val - y).shape)\n",
    "        gradient = (1/len(x)) * np.transpose(x).dot(sigmoid_val - y.reshape(len(x), 1))\n",
    "        # print(\"gradient shape is: \", gradient.shape)\n",
    "        #END\n",
    "        return gradient\n",
    "\n",
    "    def batch_update(self, batch_x, batch_y):\n",
    "        \"\"\"\n",
    "        Single self.w update using the batch.\n",
    "        :param batch_x: array of features (includes the constant feature at column 0), of shape (batch_size, num_features + 1)\n",
    "        :param batch_y: array of target values, shape (batch_size,)\n",
    "        :return: nothing\n",
    "        \"\"\"\n",
    "\n",
    "        # Workspace 2.3\n",
    "        #BEGIN \n",
    "        # code here\n",
    "        # print(\"weight shape is: \", self.w.shape)\n",
    "        self.w -= self.eta * self.compute_gradient(batch_x, batch_y)\n",
    "        #END\n",
    "\n",
    "    def fit(self, X, y, epochs=1, batch_size=1, validation_X=None, validation_y=None):\n",
    "        \"\"\"\n",
    "        train the LogisticRegression\n",
    "        :param X: training features, shape (num_samples, num_features)\n",
    "        :param y: training labels, shape (num_samples,)\n",
    "        :param epochs: number of epochs, integer\n",
    "        :param batch_size: size of batch for gradient update, 1 for SGD\n",
    "        :param validation_X: validation rows, should default to training data if not provided\n",
    "        :param validation_y: validation labels\n",
    "        :return: recall score at the end of each epoch on validation data\n",
    "        \"\"\"\n",
    "\n",
    "        if validation_X is None:\n",
    "            validation_X, validation_y = X, y\n",
    "        metrics = []\n",
    "        # Workspace  2.4\n",
    "        # TO DO: Process X to append the zero-th constant column and call self.optimize\n",
    "        # TO DO: Compute average recall on the validation data at the end of each epoch\n",
    "        # HINT: make sure to initialize w\n",
    "        #BEGIN \n",
    "        # code here\n",
    "        X = np.concatenate([np.ones((X.shape[0], 1)), X], axis=1)  # We append zero-th column\n",
    "        self.w = np.full((X.shape[1], 1), (1/len(X))) # initialize w\n",
    "        for _ in range(1, epochs+1):\n",
    "            self.optimize(X, y, batch_size)\n",
    "            y_hat = self.predict(validation_X)\n",
    "            metrics.append(recall_score(validation_y, y_hat))\n",
    "        #END\n",
    "        return np.array(metrics)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        :param X: features array, shape (num_samples, num_features) (without the constant column)\n",
    "        :return: predicted binary label, shape (num_samples,)\n",
    "        \"\"\"\n",
    "        # Workspace 2.5\n",
    "        y_hat = np.zeros((X.shape[0],))\n",
    "        X = np.concatenate([np.ones((X.shape[0], 1)), X], axis=1)  # We append zero-th column\n",
    "        #BEGIN \n",
    "        # code here\n",
    "        y_hat = self.sigmoid(X)\n",
    "        y_hat[y_hat >= 0.5] = 1\n",
    "        y_hat[y_hat < 0.5] = 0\n",
    "        #END\n",
    "        return y_hat\n",
    "\n",
    "    def optimize(self, X, y, batch_size):\n",
    "        \"\"\"\n",
    "        Perform one epoch batch gradient on shuffled data\n",
    "        :param X: np.array of shape (num_samples, num_features +1), The training data with zero-th column appended\n",
    "        :param y: target values of shape (num_samples,)\n",
    "        :param batch_size: batch_size of the batch_update\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        indices = np.random.permutation(len(X))\n",
    "        for i in range(0, X.shape[0], batch_size):\n",
    "            batch_x = X[indices[i:i + batch_size]]\n",
    "            batch_y = y[indices[i:i + batch_size]]\n",
    "            self.batch_update(batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "y_hat = np.zeros((X.shape[0],))\n",
    "print(y_hat.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2 0.2 0.2 0.2 0.2 0.2]\n",
      "[0.2 0.2 0.2 0.2 0.2 0.2]\n"
     ]
    }
   ],
   "source": [
    "w = np.full((6, ), (1/5))\n",
    "print(w)\n",
    "# q = np.full((5, ), (1/2))\n",
    "# print(w+q)\n",
    "# print(w)\n",
    "# print(w[1:])\n",
    "print(w+ 0* w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
